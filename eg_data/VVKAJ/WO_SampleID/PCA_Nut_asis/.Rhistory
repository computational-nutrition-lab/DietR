SubsetColumns(data = totals_selected, start.col = "F_TOTAL", end.col = "A_DRINKS")
# Pick up only the columns with non-zero variance, in order to run PCA, cluster analysis etc.
# The removed columns will be shown if any.
KeepNonZeroVarColumns(data = subsetted)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Cat_asis.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation by using
# the CollapseByCorrelation function.
SaveCorrMatrix(x=cc, out.fn = "VVKAJ_Tot_m_QCed_Cat_asis_corr_matrix.txt")
# ===============================================================================================================
# FOOD CATEGORIES: Take average of each user across all days
# ===============================================================================================================
# Specify the data to be used, category to group by, and the range of columns (variables)
# to calculate the means of each variable.
# Food items analysis --> start.col = "F_TOTAL", end.col = "A_DRINKS"
AverageBy(data= totals_selected, by= "UserName", start.col= "F_TOTAL", end.col= "A_DRINKS")
# Save the averaged results.
write.table(x=meansbycategorydf, "VVKAJ_Tot_m_QCed_Cat_ave_allvar.txt", sep="\t", row.names=F, quote=F)
# The column names should be UserName + start.col-end.col.
colnames(meansbycategorydf)
# The 'UserName' column has the users to calculate means for.
meansbycategorydf$UserName
# Pick up only the columns with non-zero variance, in order to run PCA and cluster analysis etc.
# The removed columns will be shown if any.
# [,-1] is to exclude the UserName columns that is not numeric and not used for variance calculation.
KeepNonZeroVarColumns(data = meansbycategorydf[, -1])
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Collapse variables by correlation: take only one variables if they are highly correlated.
cbc_res <- CollapseByCorrelation(x = subsetted_non0var, min.cor = 0.75,
select.rep.fcn = 'mean', verbose = T)
# Filter out highly correlated variables from the original dataset.
selected_variables <- subsetted_non0var[, cbc_res$reps]
# Check the name of the original and filtered variables.
# Among the variables in the same group, the one with the highest variance is kept
#  (according to the explanation above.)
# filtered
head(selected_variables, 1)
dim( selected_variables)
# original
head(subsetted_non0var, 1)
dim( subsetted_non0var)
# ---------------------------------------------------------------------------------------------------------------
# Save the selected_variables as a .txt file. This will be the input for clustering analyses.
write.table(x=selected_variables, file="VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", row.names=F, quote=F)
# ---------------------------------------------------------------------------------------------------------------
# Save the correlation matrix for record in the results folder.
# cc is the correlation matrix produced when variables are collapsed by correlation by using
# the CollapseByCorrelation function.
SaveCorrMatrix(x=cc, out.fn = "VVKAJ_Tot_m_QCed_Cat_ave_corr_matrix.txt")
# ===============================================================================================================
# Come back to the main directory
setwd(main_wd)
# Import source code to run the analyses to follow.
# source("lib/specify_dir_and_check_col.R")
# source("lib/prep_data_for_clustering.R")
source("lib/PCA.R")
# Define ggplot themes to use in creating plots.
library(ggplot2)
theme_set(theme_bw(base_size = 14))
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# Load Nut_asis data.
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Load Nut_asis data.
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_asis
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Nut_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Load Nut_asis data.
Tot_m_QCed_Nut_asis <- read.table(file="VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_asis
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Nut_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Combine the input (totals before processing) with all the variables and the PC results.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# Load Nut_ave data.
Tot_m_QCed_Nut_ave <- read.table(file="VVKAJ_Tot_m_QCed_Nut_ave.txt", sep="\t", header=T)
meansbycategorydf
# Load Nut_ave data.
Tot_m_QCed_Nut_ave <- read.table(file="VVKAJ_Tot_m_QCed_Nut_ave_subset.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_ave
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
# Input is your items/totals input file before any prep for clustering, from which you derived the input for the PCA.
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix)
res_dir
# Specify the directory (folder) to save the results.
res_dir = "Nut_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Combine the input (totals before processing) with all the variables and the PC results.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# Load Nut_ave data.
Tot_m_QCed_Nut_ave <- read.table(file="VVKAJ_Tot_m_QCed_Nut_ave_subset.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Nut_ave
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Nut_ave_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Nut_ave"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
# Input is your items/totals input file before any prep for clustering, from which you derived the input for the PCA.
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix)
# Combine the input (totals before processing) with all the variables and the PC results.
# In the case of aveaged totals data / user, the input file used here is xxx_ave_allvar.txt, which
# has all the variables before filtering out by correlation or zero variance.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed_Nut_ave_allvar.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# Load Cat_asis data.
Tot_m_QCed_Cat_asis <- read.table(file="VVKAJ_Tot_m_QCed_Cat_asis.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Cat_asis
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Cat_asis_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Cat_asis"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix )
# Combine the input (totals before processing) with all the variables and the PC results.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# Load Cat_ave data.
Tot_m_QCed_Cat_ave <- read.table(file="VVKAJ_Tot_m_QCed_Cat_ave_subset.txt", sep="\t", header=T)
# Name your input data.
pca_input <- Tot_m_QCed_Cat_ave
# Ensure your input file has the correct number of rows and columns.
dim(pca_input)
# Perform PCA with the subset data, scaled.
scaled_pca <- prcomp(x=pca_input, scale = TRUE)
# Specify the directory (folder) to save the results.
res_dir = "Cat_ave_PCA"
# Specify the prefix of filenames to be saved.
res_prefix = "VVKAJ_Cat_ave"
# Perform PCA and save the results in a specified folder (out.dir) and a prefix (out.prefix).
# Input is your items/totals input file before any prep for clustering, from which you derived the input for the PCA.
PerformPCA(pca.data=pca_input, pca.result=scaled_pca, out.dir= res_dir, out.prefix= res_prefix)
# Combine the input (totals before processing) with all the variables and the PC results.
# In the case of aveaged totals data / user, the input file used here is xxx_ave_allvar.txt, which
# has all the variables before filtering out by correlation or zero variance.
SaveInputAndPCs(input="VVKAJ_Tot_m_QCed_Cat_ave_allvar.txt", pca.results = scaled_pca,
out.dir= res_dir, out.prefix= res_prefix)
# ===============================================================================================================
# Come back to the main directory
setwd(main_wd)
# ========================================================================================
# Look at the PCA results in detail
# Created on 07/15/2022 by Rie Sadohara
# Version 1
# ========================================================================================
#
# Define ggplot2 themes
library(ggplot2)
# Theme black and white, with the base font size 14: change if necessary.
theme_set(theme_bw(base_size = 14))
# No gridlines inside charts
no_grid <- theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank())
# Insert some space between axes and axes labels.
space_axes <- theme(axis.title.x = element_text(margin=margin(t = 8, r = 0, b = 0, l = 0) ),
axis.title.y = element_text(margin=margin(t = 0, r = 10, b = 0, l = 0) ) )
# ---------------------------------------------------------------------------------------------------------------
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/Nut_asis_PCA")
# Load the PCA result
pcares <- read.table("VVKAJ_Nut_asis_PCs.txt", sep="\t", header=T)
head(pcares,1)
# Load the variance explained by each PC.
PC_var_exp <- read.table("VVKAJ_Nut_asis_PC_var_explained.txt", sep="\t", header=T)
head(PC_var_exp)
# ---------------------------------------------------------------------------------------------------------------
# Create a biplot with the users colored by different metadata.
# By Diet
metadata_diet <- read.table("clipboard", sep="\t", header=T)
colnames(PC_var_exp)
colnames(metadata_diet)
head(pcares,1)
# ---------------------------------------------------------------------------------------------------------------
# Create a biplot with the users colored by different metadata.
# Load the metadata
metadata <- read.table("../ind_metadata.txt")
metadata
# ---------------------------------------------------------------------------------------------------------------
# Create a biplot with the users colored by different metadata.
# Load the metadata
metadata <- read.table("../ind_metadata.txt", header=T)
metadata
head(pcares,1)
# Add metadata to PCA result so that individuals can be plotted accordig to their metadata.
pcares_diet <- merge(pcares, metadata, by="UserName", all.x=T)
colnames(metadata_diet)
colnames(pcares_diet)
head(pcares,1)
pcares_diet
# ---------------------------------------------------------------------------------------------------------------
# Plot
ggplot(pcares, aes(x=PC1, y=PC2, color=Diet, fill=Diet)) +
geom_point(size=3) +
no_grid + space_axes +
scale_fill_manual(values = distinct100colors) +
scale_color_manual(values = distinct100colors) +
scale_x_continuous(expand = expansion(mult=c(0.1, 0.1))) + # give some space on the lower and the upper limits of X (to fit text).
scale_y_continuous(expand = expansion(mult=c(0.1, 0.1))) + # give some space on the lower and the upper limits of Y (to fit text).
labs(x = paste("PC1 (", round(PC_var_exp[1,2]*100, 2), "%)", sep=""),
y = paste("PC2 (", round(PC_var_exp[2,2]*100, 2), "%)", sep=""))
# Name your main directory for future use.
main_wd <- file.path(getwd())
# Come back to the main directory
setwd(main_wd)
# Import source code to run the analyses to follow.
# source("lib/load_and_check.R") # nonexistent
source("lib/k-means.R")
# Come back to the main directory
setwd(main_wd)
# Come back to the main directory
setwd(main_wd)
# Import source code to run the analyses to follow.
# source("lib/load_and_check.R") # nonexistent
source("lib/k-means.R")
setwd("~/GitHub/dietary_patterns")
# Name your main directory for future use.
main_wd <- file.path(getwd())
# Come back to the main directory
setwd(main_wd)
# Import source code to run the analyses to follow.
# source("lib/load_and_check.R") # nonexistent
source("lib/k-means.R")
# ---------------------------------------------------------------------------------------------------------------
# Specify the directory where the data is.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/")
# Define your input file. Need to scale it to accommodate measurements in different units.
# Average data may not have enough users for this.
selected_variables <- read.table("VVKAJ_Tot_m_QCed_Nut_asis.txt", sep="\t", header=T)
# Check the column names (variables)
colnames(selected_variables)
# Scale the variables and define it as an input for k-means analysis.
kmeans_input <- scale(selected_variables) # correlated variables removed.
# Set your ggplot2 theme.
require(ggplot2)
theme_set(theme_bw(base_size = 14))
# Set seed for consistent results.
set.seed(123)
# ---------------------------------------------------------------------------------------------------------------
# Use the elbow method to find the ideal K. K cannot be larger than the number of datapoints (rows) in input.
ElbowMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# Or if the factoextra package does not work for some reason, there is a way to only use the
# cluster package.
SilhouetteMethod(k.values = 2:9)
# ---------------------------------------------------------------------------------------------------------------
# Use the Gap statistic method to find the ideal K. The highest K is the optimum K.
GapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Use the Gap statistic method to find the ideal K. The highest K is the optimum K.
GapMethod(k.values = 1:15)
# Or use the factoextra package to use the Gap statistic method.
FactoextraGapMethod(k.values = 1:15)
# Or use the factoextra package to use the Gap statistic method.
FactoextraGapMethod(k.values = 1:15)
# Or use the factoextra package to use the Gap statistic method.
FactoextraGapMethod(k.values = 1:15)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 4)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
One_K(myK = 5)
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with one specified k.
oneKplot <- One_K(myK = 5)
oneKplot
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
multipleKplots <- MultipleK(myKs = c(3,4,5,6))
multipleKplots
# ---------------------------------------------------------------------------------------------------------------
# Loop through multiple Ks
MultipleK <- function(myKs){
plots <- list()
km_results_mult <- list()
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
# k-means analysis
km_results_mult[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
# Define title for each K
plot_title <- paste("K=", myKs[i], sep = "")
# Plot
plots[[i]] = factoextra::fviz_cluster(km_results_mult[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10,
main = plot_title,
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) )
}
# Install the gridExtra package if needed.
if(!require("gridExtra"))install.packages("gridExtra")
if(length(myKs)==2){
gridExtra::grid.arrange(plots[[1]], plots[[2]], nrow = round(length(myKs)/2))
}
else if(length(myKs)==3){
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], nrow = round(length(myKs)/2))
}
else if(length(myKs)==4){
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = round(length(myKs)/2))
}
else{
cat("Only 2-4 plots can be created at one time.", "\n",
"Please enter 2-4 K values and run again.")
}
}
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
multipleKplots <- MultipleK(myKs = c(3,4,5,6))
# ---------------------------------------------------------------------------------------------------------------
# Loop through multiple Ks
MultipleK <- function(myKs){
plots <- list()
km_results_mult <- list()
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
# k-means analysis
km_results_mult[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
# Define title for each K
plot_title <- paste("K=", myKs[i], sep = "")
# Plot
plots[[i]] = factoextra::fviz_cluster(km_results_mult[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10,
main = plot_title,
theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank()) )
}
# Install the gridExtra package if needed.
if(!require("gridExtra"))install.packages("gridExtra")
if(length(myKs)==2){
gridExtra::grid.arrange(plots[[1]], plots[[2]], nrow = round(length(myKs)/2))
}
else if(length(myKs)==3){
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], nrow = round(length(myKs)/2))
}
else if(length(myKs)==4){
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = round(length(myKs)/2))
}
else{
cat("Only 2-4 plots can be created at one time.", "\n",
"Please enter 2-4 K values and run again.")
}
}
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
multipleKplots <- MultipleK(myKs = c(3,4,5,6))
# ---------------------------------------------------------------------------------------------------------------
# Loop through multiple Ks
MultipleK <- function(myKs){
plots <- list()
km_results_mult <- list()
# Perform the k-means analysis, with the optimum number you found above as the 'centers'.
for(i in 1:length(myKs)){
# k-means analysis
km_results_mult[[i]] <- kmeans(x=kmeans_input, centers = myKs[i], nstart = 25)
# Define title for each K
plot_title <- paste("K=", myKs[i], sep = "")
# Plot
plots[[i]] = factoextra::fviz_cluster(km_results_mult[[i]],
data = kmeans_input,
ellipse = T, ellipse.alpha = 0.1,
ggtheme = theme_bw(base_size = 10),
repel = F, labelsize = 10,
main = plot_title )
}
# Install the gridExtra package if needed.
if(!require("gridExtra"))install.packages("gridExtra")
if(length(myKs)==2){
gridExtra::grid.arrange(plots[[1]], plots[[2]], nrow = round(length(myKs)/2))
}
else if(length(myKs)==3){
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], nrow = round(length(myKs)/2))
}
else if(length(myKs)==4){
gridExtra::grid.arrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]], nrow = round(length(myKs)/2))
}
else{
cat("Only 2-4 plots can be created at one time.", "\n",
"Please enter 2-4 K values and run again.")
}
}
# ---------------------------------------------------------------------------------------------------------------
# Perform k-means analysis with multiple (2-4) Ks, and plot them in one window.
multipleKplots <- MultipleK(myKs = c(3,4,5,6))
multipleKplots
# Define the folder to save the chart.
SpecifyDataDirectory(directory.name = "eg_data/VVKAJ/Nut_asis_PCA")
# Check the column names (variables)
colnames(selected_variables)
# Save the plot as a PDF file.
ggsave("VKAJ_Tot_m_QCed_Nut_asis_K5.pdf", oneKplot,
device="pdf", width=5, height=5, units="in")
# Save the plot as a PDF file.
ggsave("VKAJ_Tot_m_QCed_Nut_asis_K5.pdf", oneKplot,
device="pdf", width=4, height=4, units="in")
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
silhouettechart <- factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
silhouettechart
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
silhouettechart <- factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
silhouettechart
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
silhouettechart <- factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
# ---------------------------------------------------------------------------------------------------------------
# Use the Silhouette method to find the ideal K. This uses the cluster and factoextra package.
silhouettechart <- factoextra::fviz_nbclust(kmeans_input, kmeans, method="silhouette")
silhouettechart
